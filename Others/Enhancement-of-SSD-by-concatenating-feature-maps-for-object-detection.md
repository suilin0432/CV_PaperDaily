|  创建人   |  知乎论文阅读专栏 | 个人博客 | 其他相关链接 |
|  ----  | ----  | ----  | ----  |
| ming71  | [论文笔记入口](https://zhuanlan.zhihu.com/c_1113860303082704896) | [chaser](https://ming71.github.io/) |   [CSDN](https://blog.csdn.net/mingqi1996) 

<span id="inline-blue">论文发布日期：2017.5 [BMVC]<p/span>

## 1. Motivation
* SSD的小尺度目标检测效果不好  
* 不同特征的相关和利用  

&emsp;&emsp;不同尺度的特征之间具有相关性，而SSD的多尺度独立预测没有考虑这一关联。<!-- more -->
<center><img src="http://chaserblog.test.upcdn.net/blogs/paper/RSSD/exp1.png" alt="" style="width:70%" /></center>

&emsp;&emsp;从实际检测角度来看，即使是FPN也有这种问题，应该也是anchor多尺度检测的inconsistency导致多尺度信息的割裂和检测的不完整性；从原因分析来看，<u>这种局部重复框多出现于大物体，因为大尺度特征图的大物体很可能在小尺度特征图上也有响应（小物体就不会了，因为到了后面就很小了）</u>  
&emsp;&emsp;问题：**那么base feature是不是就不存在这个问题？或者会缓和一些？** 

## 2. Rainbow concatenation

### 2.1 三种组合方式        
<center><img src="http://chaserblog.test.upcdn.net/blogs/paper/RSSD/str1.png" alt="" style="width:80%" /></center>

&emsp;&emsp;pooling是一种自底向上的信息流（类似PANet，但是别人是在自顶向下后再加强的）；deconvolution是自顶向下的信息流（类似FPN）；最后是本文采用的方式——**信息双向流动**，给各个层带来不同尺度的信息。        

### 2.2 Method
* 双向的信息流动  
&emsp;&emsp;自顶向下通过转置卷积上采样，自底向上通过池化下采样，在concatenate之前进行**BN归一化**，避免不同尺度信息出现被覆盖和被淹没。

* 参数共享  
&emsp;&emsp;巧妙设置拼接的通道数（512, 1024, 512, 256, 256, and 256 channels），使得每个level输出的通道相同均为2816，从而可以使用同一个分类网络来训练（如VGG使用两个3x3卷积，深度相同就可以共享），不同尺度分类网络的参数可以share。share的优势在于：1.降低过拟合风险 2.小数据集或样本不足数据集有优势 3.速度快          
&emsp;&emsp;关于这个share参数说明一点，一般FPN都是不share的，比如YOLOv3，但是FPN的实验中也做了share参数的情况，发现效果依然很好（作者设置256通道数相同的初衷也是因为传统的图像金字塔中就是share参数的）。而且多尺度检测头上进行参数共享应该是符合MTL原则的，理论上性能不会太差。  

## 3. Experiments
<center><img src="http://chaserblog.test.upcdn.net/blogs/paper/RSSD/exp2.png" alt="" style="width:80%" /></center>

* 和SSD比速度换精度，1/3的速度损失换取VOC mAP 80的一个涨点，不算惊艳，可能某个地方还存在问题，或者说这个idea并不是很关键，没有触及特征融合的敏感点。
* 很奇怪的是，单独使用pool或者deconv传递信息会使mAP下降较多。作者的解释是由于参数共享，其他尺度的样本会干扰本尺度的学习效果。这显然很勉强，首先R-SSD就没影响；其次，pooling就是类似FPN的操作，FPN中即使参数共享效果也很好，那么这里不该有这种结果。
* 个人推测有两种可能：（1）尺度跨越太大，无法放到一个框架下共享参数。TridentNet三个感受野尺度都会掉一点精度，这里居然六个（2）共享参数的特征层选择不合理，最小的三个是1*1 3*3 5*5，显然细节信息不够的，融合可能存在语义分歧，应该本着不进行大跨度融合的原则进行选择。
* 召回率来看都是高于SSD的（但是精度呢？？）于是又读了下FPN论文，发现FPN也是召回率全面高，而大目标的提升远小于中小目标。这里也是一样，所以猜测精度也和FPN一样面临下降的问题，作者所以没贴。

## **Conclusion**
* **提出了一个很好的视角**，处理的方式虽然不算细致，但是比较合理，可效果不算特别出彩。
* 多尺度信息在融合前可以进行一个**BN归一化，避免不同尺度信息的淹没**
* 总结：这篇文章的特征融合思路是目前很多比较主流的方法都或多或少应用过的，就我的目前理解来看，工作也很完备，但是结果而言并不算十分出众。可能是由于融合方式的不合理，或者说这种思路没有触及信息流动的根本敏感点（即使是不同尺度特征图，其上的特征也是有很大冗余的，所以很多看似合理的工作只能是无关痛痒的锦上添花，这一点在FSSD的实验中也有体现）

<br>


<hr />
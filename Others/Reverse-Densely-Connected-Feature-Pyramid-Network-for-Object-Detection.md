|  创建人   |  知乎论文阅读专栏 | 个人博客 | 其他相关链接 |
|  ----  | ----  | ----  | ----  |
| ming71  | [论文笔记入口](https://zhuanlan.zhihu.com/c_1113860303082704896) | [chaser](https://ming71.github.io/) |   [CSDN](https://blog.csdn.net/mingqi1996) 

<span id="inline-blue">论文发布日期：2018.11.12 [ACCV]<p/span>

&emsp;&emsp;这篇文章的idea正好是我前段时间在实验探索和改进的，看来已经有人做了，连名字都和我自己起的Dense FPN差不多....真的是服了...本文观点可圈可点，从论文本身来看写作很一般，实验设置和逻辑证明也不充分，但motivation和改进策略尚有看的地方，辩证地看。
<!-- more -->
## 1. Motivation


* FPN结构的高层语义信息通过top-down向下传递时<u>可能面临信息的丢失（递归传递会稀释抽象特征）</u>问题，下层真正利用到高级语义信息很难（没有说服力，此外<u>有论文指出融合过于高级语义特征可能造成语义分歧反而不好</u>，~~所以作者的出发点其实就可能有问题~~，而且没有数据支撑，更显得单薄）[自己想办法做实验证明这一点！！看看1：到底底层融合到了多少高层信息 2：高层是否重要，如果网络自己选会怎么选，选多高的]
* <u>对于多尺度的目标而言，很难决定哪些上层特征是有用的</u>（这个insight其实不错的，但是他的一股脑直接加其它层的处理方式感觉欠妥）


## 2. Related Work

还是说说自己的感受。
* 首先他说了concatenate比element wise sum好，这点应该没问题，毕竟参数多（~~当然，作者又没做实验...他到底干嘛去了...~~）。但是FPN已经被诟病（只是那些速度还行的网络会批评一下）多尺度检测速度慢，用concatenate恐怕更甚；前车之鉴的TDM选择单尺度预测，效果还行。但是可以看出，单尺度TDM也是不俗的，所以有下面的观察
* FPN系列对小目标提升是因为他只是将高层语义信息传下去了，让大尺度特征图的丰富细节信息能够work，但是对大尺度特征图而言，尽管还是有detect，却没有信息的变化，甚至由于大尺度特征图预测的引入而导致自己参数在学习过程中受影响变差。TDM也是类似，干脆聚焦于大尺度特征图了，因为这张图融合了最丰富的信息。这就带来一个问题：高层图单独使用有没有意义？（~~主要是TDM论文的实验太少了，只和baseline比，各方面效果当然都不差了...~~）
* 对相关job的点评：FPN一是传下去信息丢失，二是sum会造成语义混淆（~~不置可否~~）；TDM内存消耗大；FSSD只选用早期特征，模型表达能力受限（~~我看的好像是各个levela...~~）；STDN只用了最后一个stage的block（这点比较赞同，当时就觉得可能不妥，但是这点也不好说，因为人家的前提假设是DenseNet具有足够强的特征提取能力，这个不好验证，~~好吧我就是懒~~）


## 3. Rev-Dense FPN 

&emsp;&emsp;就不说什么了，这个结构以前就想到过，直接上图，然后谈几点自己没想到的地方：        
<center><img src="http://chaserblog.test.upcdn.net/blogs/paper/RevDenseFPN/str1.png" alt="" style="width:100%" /></center>

&emsp;&emsp;下图的具体结构中作者做了个我当时没来得及考虑的item：<u>不同融合层的通道数设置</u>。这里作者设置的注意k1略大于k2，比较好理解，他是想保留当前特征为主体，高层特征为辅助信息强化。但是我感觉应该可以试试反过来设定。因为**底层优势特征图比较大细节丰富，信息保存关键在响应尺度大，高层优势在于通道深，具有更多的组合**，也就是强语义信息，所以要保留各自特点。从高层角度出发必然不能过度减少通道深，否则**信息坍缩**就没意义了（当然你不能像resnet的2048那样太深了，如果是小网络我认为可以一试）。
<center><img src="http://chaserblog.test.upcdn.net/blogs/paper/RevDenseFPN/str2.png" alt="" style="width:80%" /></center>


## 4. Experiment

&emsp;&emsp;实验中他选取的k1=384，k2=48这个高层的48会不会影响信息的保留，我持怀疑态度。其他实验没什么特别值得期待和圈点的，下面的关于哪些层的选择实验，虽然做的很乱，但是稍微值得一看：
<center><img src="http://chaserblog.test.upcdn.net/blogs/paper/RevDenseFPN/exp1.png" alt="" style="width:60%" /></center>

&emsp;&emsp;二五行对比，多加两个中间层提高了0.5；三五行对比，多了个最高层提高0.4；<u>说明一个最高层的引入甚至可以相当于两个尺度的中间层加入。可见高层信息在特征融合中非常重要，做light 的rcnn可以考虑从这里下手；</u>  
&emsp;&emsp;再看一四行对比，第四行相比之下去加了个C1去了个C6，精度提高0.8，体现在两个可能点：低层次信息的重要性或者是太高的信息通道不够导致语义信息被破坏（考虑到特征融合是自适应学习的，不存在太大的gap问题）。具体原因这个实验看不出来。

**Conclusion**  
&emsp;&emsp;这篇论文没什么好总结的，很多是我之前实验过的内容，以及一点无关关键的观察。而且本文虽然是国家自然基金项目，但是没有见到中科院计算机视觉组在哪儿发表的，此外通过论文的实验安排，论述充分性，措辞（甚至还有错别词...）来看应该是个新手的paper，看看就行。    
&emsp;&emsp;关于motivation提出的那些问题还是值得关注的，他没解决的问题还是存在，这种思路有人做了可以放一放了，但是可以试着用其他方法去尝试解决这个问题。

<br>
<hr />